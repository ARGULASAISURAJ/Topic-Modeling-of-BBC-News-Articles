{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1><center>Topic Modeling of BBC News Articles </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#Section_1\"> Importing Required Libraries and Data</a></li>\n",
    "        <li><a href=\"#Section_2\"> Preprocessing Data Frame and creating TF-IDF Matrix</a></li>\n",
    "        <li><a href=\"#Section_3\"> LDA and LSA by Cleaning Method 1 </a> </li>\n",
    "        <li><a href=\"#Section_4\"> LDA and LSA by Cleaning Method 2</a></li>\n",
    "        <li><a href=\"#Section_5\"> LDA and LSA by Clenaing Method 3</a></li>\n",
    "        <li><a href=\"#Section_6\"> Five most common keywords across these six groups of keywords</a></li>\n",
    "        <li><a href=\"#Section_6\"> Observations</a></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"#Section_1\"> 1. Importing Required Libraries and Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import re\n",
    "from gensim.models import TfidfModel, LsiModel, CoherenceModel, LdaModel\n",
    "import numpy as np\n",
    "from gensim.corpora import Dictionary\n",
    "from textblob import TextBlob\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(\"BBC-articles.csv\") # importing csv into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Section_2\"> 2. Preprocessing Data Frame and creating TF-IDF Matrix</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#Section_2\"> Basic Cleaning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the articles by removing the punctuation, fullstops, stopwords, 'words_len>2' \n",
    "def cleantext(text):\n",
    "    text = text.strip(punctuation).lower()\n",
    "    text = re.sub(r'[!?,.\\:;\\n\\t]+', '', text)\n",
    "    word= nltk.tokenize.word_tokenize(text)#tokenization\n",
    "    word = [w for w in word if w.isalpha()]# selecting only words\n",
    "    word = [w for w in word if w not in stopwords.words('english') and len(w) > 2]#removing stopwords \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#Section_2\"> TF-IDF Matrix by different matrix</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_maker(articles,clean_method):\n",
    "    # creating a list of token of all the articles(documents)\n",
    "    token = []    \n",
    "    if clean_method==1:\n",
    "        #More cleaning with the help of lemmatizing words \n",
    "        for i in articles.index:\n",
    "            words = cleantext(articles.loc[i, 'text']) #calling basic function\n",
    "            wordnet = nltk.stem.WordNetLemmatizer() #Normalization using Lemmatization technique\n",
    "            lemmatized_words = [wordnet.lemmatize(w) for w in words] # keeping lemmatized words\n",
    "            token.append(lemmatized_words)             #appending to empty token list        \n",
    "        my_dict = Dictionary(token)  #Converting words into a dictonary Tokenization \n",
    "        return my_dict,token \n",
    "    elif clean_method==2:\n",
    "        #to exclude the top 10% of the most frequent words and words that appear less than 5 times in the documents\n",
    "        for i in articles.index:\n",
    "            words = cleantext(articles.loc[i, 'text'])\n",
    "            token.append(words) #appending to a empty token list\n",
    "        my_dict = Dictionary(token)  #Converting words into a dictonary Tokenization\n",
    "        #exclude the top 10% and words that appear less than 5 times\n",
    "        my_dict.filter_extremes(no_below=5, no_above=0.90)\n",
    "        return my_dict,token\n",
    "    elif clean_method==3:\n",
    "        #Limiting the word list with nouns\n",
    "        for i in articles.index:\n",
    "            words = cleantext(articles.loc[i, 'text'])\n",
    "            modified_text=' '.join([w for w in words])\n",
    "            blob_object = TextBlob(modified_text)\n",
    "            #Limiting the word list with nouns\n",
    "            word_list_nouns = [word for word,pos in blob_object.tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "            token.append(word_list_nouns) #apending a empty token list\n",
    "        my_dict = Dictionary(token)   #Converting words into a dictonary Tokenization\n",
    "        return my_dict,token                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#Section_2\"> Determining Max Coherence/topics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining optimum number of topics using coherence values \n",
    "def maxCoherence(corpus, isLsi,my_dict,token):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    min_topics, max_topics, step = 1, 10, 1\n",
    "    for i in range(min_topics, max_topics, step):\n",
    "        if (isLsi) :\n",
    "            model = LsiModel(corpus, id2word=my_dict, num_topics=i)\n",
    "        else:\n",
    "            model = LdaModel(corpus, id2word=my_dict, num_topics=i)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=token, dictionary=my_dict, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return coherence_values.index(max(coherence_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#Section_2\"> Dominant topic and Keyword for each article/topics</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dominant topic and corresponding keywords for each article\n",
    "def getkeywords(model, corpus): \n",
    "    # Init output\n",
    "    topickeyword_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(model[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = model.show_topic(topic_num, topn=5)\n",
    "                #topn = 5 gives top 5 kwywords \n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                topickeyword_df = topickeyword_df.append(pd.Series([topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    return(topickeyword_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#Section_2\"> Modelling Function</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_method(clean_method):\n",
    "    #convert a list of words to bag of words\n",
    "    my_dict,token=tfidf_maker(articles,clean_method)\n",
    "    dtm = [my_dict.doc2bow(doc) for doc in token] #convert a list of words to bag of words\n",
    "    tfidf = TfidfModel(dtm) # TF-IDF Vectorization for the document term matrix\n",
    "    tfidf = tfidf[dtm]\n",
    "\n",
    "    # Gensim: LSI\n",
    "    lsi_model = LsiModel(corpus=tfidf, id2word=my_dict, num_topics=maxCoherence(tfidf,isLsi=True,my_dict = my_dict,token = token))\n",
    "\n",
    "    # Gensim: LDA\n",
    "    lda_model = LdaModel(corpus=tfidf, id2word=my_dict, num_topics=maxCoherence(tfidf,isLsi=False,my_dict = my_dict,token = token))\n",
    "    return lsi_model,lda_model,tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Section_2\"> 3.LSA and LDA by cleaning method 1</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model_1,lda_model_1,tfidf = models_method(1)\n",
    "# add top 5 keywords for each model into the dataframe after vectorization \n",
    "articles['LSI Clean Keywords'] = getkeywords(model=lsi_model_1, corpus=tfidf)\n",
    "articles['LDA Clean Keywords'] = getkeywords(model=lda_model_1, corpus=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>LSI Clean Keywords</th>\n",
       "      <th>LDA Clean Keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>search, blair, holmes, blog, mobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  \\\n",
       "0      tech  tv future in the hands of viewers with home th...   \n",
       "1  business  worldcom boss  left books alone  former worldc...   \n",
       "2     sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "\n",
       "                    LSI Clean Keywords                   LDA Clean Keywords  \n",
       "0  labour, election, blair, tax, brown  search, blair, holmes, blog, mobile  \n",
       "1  labour, election, blair, tax, brown  bank, dollar, player, sale, economy  \n",
       "2  labour, election, blair, tax, brown  bank, dollar, player, sale, economy  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Section_2\"> 4.LSA and LDA by cleaning method 2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model_2,lda_model_2,tfidf = models_method(2)\n",
    "# add top 5 keywords for each model into the dataframe after vectorization \n",
    "articles['LSI Clean Keywords 2'] = getkeywords(model=lsi_model_2, corpus=tfidf)\n",
    "articles['LDA Clean Keywords 2'] = getkeywords(model=lda_model_2, corpus=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>LSI Clean Keywords</th>\n",
       "      <th>LDA Clean Keywords</th>\n",
       "      <th>LSI Clean Keywords 2</th>\n",
       "      <th>LDA Clean Keywords 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>search, blair, holmes, blog, mobile</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  \\\n",
       "0      tech  tv future in the hands of viewers with home th...   \n",
       "1  business  worldcom boss  left books alone  former worldc...   \n",
       "2     sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "\n",
       "                    LSI Clean Keywords                   LDA Clean Keywords  \\\n",
       "0  labour, election, blair, tax, brown  search, blair, holmes, blog, mobile   \n",
       "1  labour, election, blair, tax, brown  bank, dollar, player, sale, economy   \n",
       "2  labour, election, blair, tax, brown  bank, dollar, player, sale, economy   \n",
       "\n",
       "                     LSI Clean Keywords 2  \\\n",
       "0  labour, blair, election, people, brown   \n",
       "1  labour, blair, election, people, brown   \n",
       "2  labour, blair, election, people, brown   \n",
       "\n",
       "                   LDA Clean Keywords 2  \n",
       "0  mobile, games, music, blair, players  \n",
       "1  mobile, games, music, blair, players  \n",
       "2  mobile, games, music, blair, players  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Section_3\"> 5.LSA and LDA by cleaning method 3</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model_3,lda_model_3,tfidf = models_method(3)\n",
    "# add top 5 keywords for each model into the dataframe after vectorization \n",
    "articles['LSI Clean Keywords 3'] = getkeywords(model=lsi_model_3, corpus=tfidf)\n",
    "articles['LDA Clean Keywords 3'] = getkeywords(model=lda_model_3, corpus=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>LSI Clean Keywords</th>\n",
       "      <th>LDA Clean Keywords</th>\n",
       "      <th>LSI Clean Keywords 2</th>\n",
       "      <th>LDA Clean Keywords 2</th>\n",
       "      <th>LSI Clean Keywords 3</th>\n",
       "      <th>LDA Clean Keywords 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>search, blair, holmes, blog, mobile</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "      <td>election, blair, government, party, people</td>\n",
       "      <td>dollar, bank, prices, growth, oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "      <td>election, blair, government, party, people</td>\n",
       "      <td>search, google, party, users, people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "      <td>election, blair, government, party, people</td>\n",
       "      <td>blair, party, election, tax, kennedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  \\\n",
       "0      tech  tv future in the hands of viewers with home th...   \n",
       "1  business  worldcom boss  left books alone  former worldc...   \n",
       "2     sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "\n",
       "                    LSI Clean Keywords                   LDA Clean Keywords  \\\n",
       "0  labour, election, blair, tax, brown  search, blair, holmes, blog, mobile   \n",
       "1  labour, election, blair, tax, brown  bank, dollar, player, sale, economy   \n",
       "2  labour, election, blair, tax, brown  bank, dollar, player, sale, economy   \n",
       "\n",
       "                     LSI Clean Keywords 2  \\\n",
       "0  labour, blair, election, people, brown   \n",
       "1  labour, blair, election, people, brown   \n",
       "2  labour, blair, election, people, brown   \n",
       "\n",
       "                   LDA Clean Keywords 2  \\\n",
       "0  mobile, games, music, blair, players   \n",
       "1  mobile, games, music, blair, players   \n",
       "2  mobile, games, music, blair, players   \n",
       "\n",
       "                         LSI Clean Keywords 3  \\\n",
       "0  election, blair, government, party, people   \n",
       "1  election, blair, government, party, people   \n",
       "2  election, blair, government, party, people   \n",
       "\n",
       "                   LDA Clean Keywords 3  \n",
       "0     dollar, bank, prices, growth, oil  \n",
       "1  search, google, party, users, people  \n",
       "2  blair, party, election, tax, kennedy  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Section_3\"> 6.Five most common keywords across these six groups of keywords</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining keywords from LSA , LDA after 3 ceaing methods into a new keyword column\n",
    "articles['keyword'] = articles[articles.columns[2:]].apply(\n",
    "    lambda x: ','.join(x.dropna().astype(str)),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To Get 5 most common keywords from all the LSI and LDA Keywords\n",
    "from collections import Counter \n",
    "for i in articles.index:\n",
    "    key_word = articles.loc[i, 'keyword']\n",
    "    key_word = key_word.split(',')\n",
    "    most_occur = Counter(key_word).most_common(5) \n",
    "    articles.loc[i, 'Top 5 Words'] = ','.join([word[0] for word in most_occur])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"#Section_3\"> To CSV </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = articles.drop(columns=['keyword']) #every keyword\n",
    "articles.to_csv('BBC_Keywords.csv',index=False,encoding='utf-8') #write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>LSI Clean Keywords</th>\n",
       "      <th>LDA Clean Keywords</th>\n",
       "      <th>LSI Clean Keywords 2</th>\n",
       "      <th>LDA Clean Keywords 2</th>\n",
       "      <th>LSI Clean Keywords 3</th>\n",
       "      <th>LDA Clean Keywords 3</th>\n",
       "      <th>Top 5 Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>search, blair, holmes, blog, mobile</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "      <td>election, blair, government, party, people</td>\n",
       "      <td>dollar, bank, prices, growth, oil</td>\n",
       "      <td>blair,labour, election, brown, people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "      <td>election, blair, government, party, people</td>\n",
       "      <td>search, google, party, users, people</td>\n",
       "      <td>blair, people,labour, election, brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "      <td>election, blair, government, party, people</td>\n",
       "      <td>blair, party, election, tax, kennedy</td>\n",
       "      <td>blair, election,labour, tax, brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>mobile, games, music, blair, players</td>\n",
       "      <td>growth, economy, film, bank, oil</td>\n",
       "      <td>dollar, bank, prices, growth, oil</td>\n",
       "      <td>blair,labour, election, brown, economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>labour, election, blair, tax, brown</td>\n",
       "      <td>bank, dollar, player, sale, economy</td>\n",
       "      <td>labour, blair, election, people, brown</td>\n",
       "      <td>film, box, oscar, office, mercedes</td>\n",
       "      <td>election, blair, government, party, people</td>\n",
       "      <td>film, attacks, turkey, glasgow, morrison</td>\n",
       "      <td>blair,labour, election, brown, people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                    LSI Clean Keywords                   LDA Clean Keywords  \\\n",
       "0  labour, election, blair, tax, brown  search, blair, holmes, blog, mobile   \n",
       "1  labour, election, blair, tax, brown  bank, dollar, player, sale, economy   \n",
       "2  labour, election, blair, tax, brown  bank, dollar, player, sale, economy   \n",
       "3  labour, election, blair, tax, brown  bank, dollar, player, sale, economy   \n",
       "4  labour, election, blair, tax, brown  bank, dollar, player, sale, economy   \n",
       "\n",
       "                     LSI Clean Keywords 2  \\\n",
       "0  labour, blair, election, people, brown   \n",
       "1  labour, blair, election, people, brown   \n",
       "2  labour, blair, election, people, brown   \n",
       "3  labour, blair, election, people, brown   \n",
       "4  labour, blair, election, people, brown   \n",
       "\n",
       "                   LDA Clean Keywords 2  \\\n",
       "0  mobile, games, music, blair, players   \n",
       "1  mobile, games, music, blair, players   \n",
       "2  mobile, games, music, blair, players   \n",
       "3  mobile, games, music, blair, players   \n",
       "4    film, box, oscar, office, mercedes   \n",
       "\n",
       "                         LSI Clean Keywords 3  \\\n",
       "0  election, blair, government, party, people   \n",
       "1  election, blair, government, party, people   \n",
       "2  election, blair, government, party, people   \n",
       "3            growth, economy, film, bank, oil   \n",
       "4  election, blair, government, party, people   \n",
       "\n",
       "                       LDA Clean Keywords 3  \\\n",
       "0         dollar, bank, prices, growth, oil   \n",
       "1      search, google, party, users, people   \n",
       "2      blair, party, election, tax, kennedy   \n",
       "3         dollar, bank, prices, growth, oil   \n",
       "4  film, attacks, turkey, glasgow, morrison   \n",
       "\n",
       "                               Top 5 Words  \n",
       "0    blair,labour, election, brown, people  \n",
       "1    blair, people,labour, election, brown  \n",
       "2       blair, election,labour, tax, brown  \n",
       "3   blair,labour, election, brown, economy  \n",
       "4    blair,labour, election, brown, people  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"#Section_3\"> 7. Observatons </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After vectorizing the text using TF-IDF vector in three different ways:\n",
    "1. normal cleaning\n",
    "2. using term frequncy\n",
    "3. part of speech as noun\n",
    "and using LSI/LSA and LDA algorithms for topic modeling.\n",
    "\n",
    "From the results - LDA model using normal cleaning has better keywords and relevant to each article.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
